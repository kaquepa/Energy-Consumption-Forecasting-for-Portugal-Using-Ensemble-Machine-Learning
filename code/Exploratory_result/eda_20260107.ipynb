{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55839278",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T09:52:26.862774Z",
     "iopub.status.busy": "2026-01-07T09:52:26.862554Z",
     "iopub.status.idle": "2026-01-07T09:52:29.889738Z",
     "shell.execute_reply": "2026-01-07T09:52:29.889421Z"
    },
    "papermill": {
     "duration": 3.033186,
     "end_time": "2026-01-07T09:52:29.890313",
     "exception": false,
     "start_time": "2026-01-07T09:52:26.857127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPLORATORY DATA ANALYSIS - Energy Consumption Forecasting for Portugal Using Ensemble Machine Learning\n",
      "================================================================================\n",
      "Dataset: data/processed/dataset_base.csv\n",
      "Output:  code/Exploratory_result\n",
      "\n",
      "Dataset: 5849 rows x 20 columns\n",
      "Period: 2010-01-01 - 2026-01-06\n",
      "Duration: 5849 days\n",
      "\n",
      "********************************************************************************\n",
      "  1. BASIC STATISTICS\n",
      "TARGET VARIABLE (corrigido_temperatura):\n",
      "  Mean:     137.60 GWh\n",
      "  Median:   138.00 GWh\n",
      "  Std Dev:  15.21 GWh\n",
      "  Min:      72.00 GWh\n",
      "  Max:      185.00 GWh\n",
      "  Range:    113.00 GWh\n",
      "  CV:       11.05%\n",
      "No missing values found!\n",
      "  float64: 18 columns\n",
      "  datetime64[ns]: 1 columns\n",
      "  int64: 1 columns\n",
      "\n",
      "NUMERICAL FEATURES: 19 columns\n",
      "\n",
      "********************************************************************************\n",
      "  3. TEMPORAL ANALYSIS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEARLY STATISTICS:\n",
      "        mean    std    min    max  count\n",
      "year                                    \n",
      "2010  142.09  16.08  108.0  178.0    364\n",
      "2011  138.49  15.55  101.0  180.0    365\n",
      "2012  133.43  13.85   99.0  162.0    366\n",
      "2013  133.85  13.17  106.0  166.0    365\n",
      "2014  134.16  13.45   99.0  166.0    365\n",
      "2015  134.50  13.16  100.0  166.0    365\n",
      "2016  134.81  13.47  101.0  166.0    366\n",
      "2017  136.49  13.49   99.0  164.0    365\n",
      "2018  138.62  14.04  105.0  168.0    365\n",
      "2019  138.48  14.20  104.0  169.0    365\n",
      "2020  134.20  17.26   98.0  175.0    366\n",
      "2021  136.16  14.49  101.0  172.0    365\n",
      "2022  138.73  14.48  104.0  168.0    365\n",
      "2023  139.20  15.99  100.0  178.0    365\n",
      "2024  142.35  16.10  108.0  183.0    366\n",
      "2025  145.73  17.26   72.0  182.0    365\n",
      "2026  158.83  18.14  137.0  185.0      6\n",
      "\n",
      "MONTHLY PATTERN:\n",
      "  Jan: 156.75 GWh\n",
      "  Feb: 150.45 GWh\n",
      "  Dec: 147.00 GWh\n",
      "  Nov: 141.20 GWh\n",
      "  Mar: 139.56 GWh\n",
      "  Jul: 136.58 GWh\n",
      "  Sep: 133.68 GWh\n",
      "  Oct: 132.10 GWh\n",
      "  Apr: 129.51 GWh\n",
      "  Jun: 129.27 GWh\n",
      "  Aug: 128.53 GWh\n",
      "  May: 126.93 GWh\n",
      "\n",
      "WEEKLY PATTERN:\n",
      "  Monday    : 140.19 GWh\n",
      "  Tuesday   : 144.07 GWh\n",
      "  Wednesday : 144.78 GWh\n",
      "  Thursday  : 144.47 GWh\n",
      "  Friday    : 143.40 GWh\n",
      "  Saturday  : 127.28 GWh\n",
      "  Sunday    : 119.02 GWh\n",
      "\n",
      "  Weekday average: 143.38 GWh\n",
      "  Weekend average: 123.15 GWh\n",
      "  Weekend reduction: 14.11%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "  4. DISTRIBUTION ANALYSIS\n",
      "NORMALITY TEST (Shapiro-Wilk):\n",
      "  Statistic: 0.9908\n",
      "  P-value: 0.0000\n",
      "  Result: Data is NOT normally distributed\n",
      "\n",
      "SHAPE METRICS:\n",
      "  Skewness: 0.1453 (Fairly symmetric)\n",
      "  Kurtosis: -0.1329 (Normal tails)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "  5. OUTLIER ANALYSIS\n",
      "IQR METHOD (1.5 × IQR):\n",
      "  Q1 (25th percentile): 128.00 GWh\n",
      "  Q3 (75th percentile): 146.00 GWh\n",
      "  IQR: 18.00 GWh\n",
      "  Lower bound: 101.00 GWh\n",
      "  Upper bound: 173.00 GWh\n",
      "\n",
      "  Low outliers:   12 (0.21%)\n",
      "  High outliers:  71 (1.21%)\n",
      "  Total:          83 (1.42%)\n",
      "\n",
      "Z-SCORE METHOD (|z| > 3):\n",
      "  Outliers: 2 (0.03%)\n",
      "\n",
      "********************************************************************************\n",
      "  6. CORRELATION ANALYSIS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 20 FEATURES CORRELATED WITH TARGET:\n",
      "   1. corrigido_temperatura                   : 1.0000\n",
      "   2. shortwave_radiation_sum                 : 0.4369\n",
      "   3. hidrica                                 : 0.4089\n",
      "   4. temperature_2m_mean                     : 0.3905\n",
      "   5. temperature_2m_max                      : 0.3897\n",
      "   6. temperature_2m_min                      : 0.3617\n",
      "   7. relative_humidity_2m_mean               : 0.3089\n",
      "   8. exportacao                              : 0.2746\n",
      "   9. gas_natural                             : 0.2396\n",
      "  10. eolica                                  : 0.2392\n",
      "  11. outra_termica                           : 0.1725\n",
      "  12. producao_bombagem                       : 0.1537\n",
      "  13. precipitation_sum                       : 0.1482\n",
      "  14. importacao                              : 0.1215\n",
      "  15. consumo_bombagem                        : 0.0925\n",
      "  16. consumo_armazenamento                   : 0.0925\n",
      "  17. wind_speed_10m_mean                     : 0.0830\n",
      "  18. biomassa                                : 0.0486\n",
      "  19. solar                                   : 0.0226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "  7. WEATHER IMPACT ANALYSIS\n",
      "Found 9 weather related features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WEATHER CORRELATIONS WITH CONSUMPTION:\n",
      "  solar                                   : -0.0226\n",
      "  temperature_2m_mean                     : -0.3905\n",
      "  temperature_2m_max                      : -0.3897\n",
      "  temperature_2m_min                      : -0.3617\n",
      "  relative_humidity_2m_mean               : +0.3089\n",
      "  precipitation_sum                       : +0.1482\n",
      "  wind_speed_10m_mean                     : +0.0830\n",
      "  shortwave_radiation_sum                 : -0.4369\n",
      "\n",
      "********************************************************************************\n",
      "EXPLORATORY DATA ANALYSIS COMPLETED SUCCESSFULLY\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "FIGSIZE_LARGE = (16, 8)\n",
    "FIGSIZE_MEDIUM = (14, 6)\n",
    "FIGSIZE_SMALL = (12, 5)\n",
    "DPI = 150\n",
    "\n",
    "class ExploratoryAnalysis:\n",
    "    def __init__(self, data_path: str, output_dir: str = 'Exploratory_result'):\n",
    "        \"\"\"Initialize EDA with dataset path and output directory.\"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load data\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        self.df['date'] = pd.to_datetime(self.df['date'])\n",
    "        \n",
    "        if 'corrigido_temperatura' in self.df.columns:\n",
    "            self.target = 'corrigido_temperatura'\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Target column not found!\\n\"\n",
    "            )\n",
    "        \n",
    "        print(f\"Dataset: {len(self.df)} rows x {len(self.df.columns)} columns\")\n",
    "        print(f\"Period: {self.df['date'].min().date()} - {self.df['date'].max().date()}\")\n",
    "        print(f\"Duration: {(self.df['date'].max() - self.df['date'].min()).days} days\")\n",
    "    \n",
    "    def section_header(self, title: str):\n",
    "        \"\"\"Print formatted section header.\"\"\"\n",
    "        print(f\"\\n{'*' * 80}\")\n",
    "        print(f\"  {title}\")\n",
    "    \n",
    "    def save_plot(self, filename: str):\n",
    "        \"\"\"Save current plot to file.\"\"\"\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        plt.savefig(filepath, dpi=DPI, bbox_inches='tight')\n",
    "        \n",
    "    def basic_statistics(self):\n",
    "        \"\"\"Generate comprehensive basic statistics.\"\"\"\n",
    "        self.section_header(\"1. BASIC STATISTICS\")\n",
    "        \n",
    "        # Target variable statistics\n",
    "        if self.target in self.df.columns:\n",
    "            print(f\"TARGET VARIABLE ({self.target}):\")\n",
    "            print(f\"  Mean:     {self.df[self.target].mean():.2f} GWh\")\n",
    "            print(f\"  Median:   {self.df[self.target].median():.2f} GWh\")\n",
    "            print(f\"  Std Dev:  {self.df[self.target].std():.2f} GWh\")\n",
    "            print(f\"  Min:      {self.df[self.target].min():.2f} GWh\")\n",
    "            print(f\"  Max:      {self.df[self.target].max():.2f} GWh\")\n",
    "            print(f\"  Range:    {self.df[self.target].max() - self.df[self.target].min():.2f} GWh\")\n",
    "            print(f\"  CV:       {(self.df[self.target].std() / self.df[self.target].mean()) * 100:.2f}%\")\n",
    "        \n",
    "    \n",
    "        # Missing values\n",
    "        missing = self.df.isnull().sum()\n",
    "        missing = missing[missing > 0].sort_values(ascending=False)\n",
    "        if len(missing) > 0:\n",
    "            for col, count in missing.head(20).items():\n",
    "                pct = (count / len(self.df)) * 100\n",
    "                print(f\"  {col}: {count} ({pct:.2f}%)\")\n",
    "        else:\n",
    "            print(\"No missing values found!\")\n",
    "        \n",
    "        # Data types\n",
    "        type_counts = self.df.dtypes.value_counts()\n",
    "        for dtype, count in type_counts.items():\n",
    "            print(f\"  {dtype}: {count} columns\")\n",
    "        \n",
    "        # Numerical columns summary\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        print(f\"\\nNUMERICAL FEATURES: {len(numeric_cols)} columns\")\n",
    "        return self.df.describe()\n",
    "    \n",
    "\n",
    "    def temporal_analysis(self):\n",
    "        \"\"\"Analyze temporal patterns and trends.\"\"\"\n",
    "        self.section_header(\"3. TEMPORAL ANALYSIS\")\n",
    "        \n",
    "        # Create temporal features\n",
    "        self.df['year'] = self.df['date'].dt.year\n",
    "        self.df['month'] = self.df['date'].dt.month\n",
    "        self.df['day_of_week'] = self.df['date'].dt.dayofweek\n",
    "        self.df['day_name'] = self.df['date'].dt.day_name()\n",
    "        self.df['is_weekend'] = self.df['day_of_week'].isin([5, 6])\n",
    "        \n",
    "        # Time series plot\n",
    "        fig, axes = plt.subplots(2, 1, figsize=FIGSIZE_LARGE, sharex=True)\n",
    "        \n",
    "        # Raw time series\n",
    "        axes[0].plot(self.df['date'], self.df[self.target], alpha=0.7, linewidth=0.8)\n",
    "        axes[0].set_title('Daily Energy Consumption Time Series', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_ylabel('Consumption (GWh)', fontsize=12)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rolling means\n",
    "        rolling_30 = self.df[self.target].rolling(window=30, center=True).mean()\n",
    "        rolling_365 = self.df[self.target].rolling(window=365, center=True).mean()\n",
    "        \n",
    "        axes[1].plot(self.df['date'], rolling_30, label='30-day MA', linewidth=1.5)\n",
    "        axes[1].plot(self.df['date'], rolling_365, label='365-day MA', linewidth=2)\n",
    "        axes[1].set_title('Moving Averages', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Date', fontsize=12)\n",
    "        axes[1].set_ylabel('Consumption (GWh)', fontsize=12)\n",
    "        axes[1].legend(loc='best')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self.save_plot('target_timeseries.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Yearly statistics\n",
    "        print(\"YEARLY STATISTICS:\")\n",
    "        yearly_stats = self.df.groupby('year')[self.target].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "        print(yearly_stats.round(2))\n",
    "        \n",
    "        # Monthly pattern\n",
    "        print(\"\\nMONTHLY PATTERN:\")\n",
    "        monthly_avg = self.df.groupby('month')[self.target].mean().sort_values(ascending=False)\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                       'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        for month, value in monthly_avg.items():\n",
    "            print(f\"  {month_names[month-1]}: {value:.2f} GWh\")\n",
    "        \n",
    "        # Weekly pattern\n",
    "        print(\"\\nWEEKLY PATTERN:\")\n",
    "        weekly_avg = self.df.groupby('day_name')[self.target].mean()\n",
    "        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        for day in day_order:\n",
    "            if day in weekly_avg.index:\n",
    "                print(f\"  {day:10s}: {weekly_avg[day]:.2f} GWh\")\n",
    "        \n",
    "        # Weekend vs Weekday\n",
    "        weekend_avg = self.df[self.df['is_weekend']][self.target].mean()\n",
    "        weekday_avg = self.df[~self.df['is_weekend']][self.target].mean()\n",
    "        print(f\"\\n  Weekday average: {weekday_avg:.2f} GWh\")\n",
    "        print(f\"  Weekend average: {weekend_avg:.2f} GWh\")\n",
    "        print(f\"  Weekend reduction: {((weekday_avg - weekend_avg) / weekday_avg * 100):.2f}%\")\n",
    "        \n",
    "        # Monthly boxplot \n",
    "        fig, ax = plt.subplots(figsize=FIGSIZE_MEDIUM)\n",
    "        sns.boxplot(data=self.df, x='month', y=self.target, ax=ax, palette='Set3')\n",
    "        ax.set_title('Monthly Consumption Distribution', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Month', fontsize=12)\n",
    "        ax.set_ylabel('Consumption (GWh)', fontsize=12)\n",
    "        ax.set_xticklabels(month_names)\n",
    "        plt.tight_layout()\n",
    "        self.save_plot('monthly_consumption.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Weekly pattern plot\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        weekly_data = self.df.groupby('day_of_week')[self.target].mean()\n",
    "        ax.bar(range(7), weekly_data.values, color=['#3498db']*5 + ['#e74c3c']*2)\n",
    "        ax.set_xticks(range(7))\n",
    "        ax.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "        ax.set_title('Average Consumption by Day of Week', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('Consumption (GWh)', fontsize=12)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add values on bars\n",
    "        for i, v in enumerate(weekly_data.values):\n",
    "            ax.text(i, v + 1, f'{v:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self.save_plot('weekly_consumption.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def distribution_analysis(self):\n",
    "        \"\"\"Analyze distribution of target variable.\"\"\"\n",
    "        self.section_header(\"4. DISTRIBUTION ANALYSIS\")\n",
    "        \n",
    "        # Normality test\n",
    "        sample_size = min(5000, len(self.df[self.target].dropna()))\n",
    "        statistic, p_value = stats.shapiro(self.df[self.target].dropna().sample(sample_size))\n",
    "        print(f\"NORMALITY TEST (Shapiro-Wilk):\")\n",
    "        print(f\"  Statistic: {statistic:.4f}\")\n",
    "        print(f\"  P-value: {p_value:.4f}\")\n",
    "        if p_value < 0.05:\n",
    "            print(f\"  Result: Data is NOT normally distributed\")\n",
    "        else:\n",
    "            print(f\"  Result: Data is normally distributed\")\n",
    "        \n",
    "        # Skewness and Kurtosis\n",
    "        skew = stats.skew(self.df[self.target].dropna())\n",
    "        kurt = stats.kurtosis(self.df[self.target].dropna())\n",
    "        print(f\"\\nSHAPE METRICS:\")\n",
    "        print(f\"  Skewness: {skew:.4f} \", end='')\n",
    "        if abs(skew) < 0.5:\n",
    "            print(\"(Fairly symmetric)\")\n",
    "        elif skew > 0:\n",
    "            print(\"(Right-skewed)\")\n",
    "        else:\n",
    "            print(\"(Left-skewed)\")\n",
    "        \n",
    "        print(f\"  Kurtosis: {kurt:.4f} \", end='')\n",
    "        if abs(kurt) < 0.5:\n",
    "            print(\"(Normal tails)\")\n",
    "        elif kurt > 0:\n",
    "            print(\"(Heavy tails)\")\n",
    "        else:\n",
    "            print(\"(Light tails)\")\n",
    "        \n",
    "        # Distribution plot\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Histogram with KDE\n",
    "        axes[0, 0].hist(self.df[self.target].dropna(), bins=50, alpha=0.7, \n",
    "                       color='skyblue', edgecolor='black')\n",
    "        axes[0, 0].axvline(self.df[self.target].mean(), color='red', linestyle='--', \n",
    "                          linewidth=2, label=f'Mean: {self.df[self.target].mean():.1f}')\n",
    "        axes[0, 0].axvline(self.df[self.target].median(), color='green', linestyle='--', \n",
    "                          linewidth=2, label=f'Median: {self.df[self.target].median():.1f}')\n",
    "        axes[0, 0].set_title('Distribution with Mean/Median', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Consumption (GWh)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # KDE plot\n",
    "        self.df[self.target].dropna().plot(kind='kde', ax=axes[0, 1], linewidth=2, color='darkblue')\n",
    "        axes[0, 1].set_title('Kernel Density Estimate', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Consumption (GWh)')\n",
    "        axes[0, 1].set_ylabel('Density')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Q-Q plot\n",
    "        stats.probplot(self.df[self.target].dropna(), dist=\"norm\", plot=axes[1, 0])\n",
    "        axes[1, 0].set_title('Q-Q Plot (Normal Distribution)', fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Box plot\n",
    "        axes[1, 1].boxplot(self.df[self.target].dropna(), vert=True, patch_artist=True,\n",
    "                          boxprops=dict(facecolor='lightblue', color='navy'),\n",
    "                          medianprops=dict(color='red', linewidth=2),\n",
    "                          whiskerprops=dict(color='navy'),\n",
    "                          capprops=dict(color='navy'))\n",
    "        axes[1, 1].set_title('Box Plot', fontsize=12, fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Consumption (GWh)')\n",
    "        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self.save_plot('target_distribution.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def outlier_analysis(self):\n",
    "        \"\"\"Comprehensive outlier detection and analysis.\"\"\"\n",
    "        self.section_header(\"5. OUTLIER ANALYSIS\")\n",
    "        \n",
    "        # IQR method\n",
    "        Q1 = self.df[self.target].quantile(0.25)\n",
    "        Q3 = self.df[self.target].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers_low = self.df[self.df[self.target] < lower_bound]\n",
    "        outliers_high = self.df[self.df[self.target] > upper_bound]\n",
    "        total_outliers = len(outliers_low) + len(outliers_high)\n",
    "        \n",
    "        print(f\"IQR METHOD (1.5 × IQR):\")\n",
    "        print(f\"  Q1 (25th percentile): {Q1:.2f} GWh\")\n",
    "        print(f\"  Q3 (75th percentile): {Q3:.2f} GWh\")\n",
    "        print(f\"  IQR: {IQR:.2f} GWh\")\n",
    "        print(f\"  Lower bound: {lower_bound:.2f} GWh\")\n",
    "        print(f\"  Upper bound: {upper_bound:.2f} GWh\")\n",
    "        print(f\"\\n  Low outliers:  {len(outliers_low):3d} ({len(outliers_low)/len(self.df)*100:.2f}%)\")\n",
    "        print(f\"  High outliers: {len(outliers_high):3d} ({len(outliers_high)/len(self.df)*100:.2f}%)\")\n",
    "        print(f\"  Total:         {total_outliers:3d} ({total_outliers/len(self.df)*100:.2f}%)\")\n",
    "        \n",
    "        # Z-score method\n",
    "        z_scores = np.abs(stats.zscore(self.df[self.target].dropna()))\n",
    "        outliers_z = len(z_scores[z_scores > 3])\n",
    "        print(f\"\\nZ-SCORE METHOD (|z| > 3):\")\n",
    "        print(f\"  Outliers: {outliers_z} ({outliers_z/len(self.df)*100:.2f}%)\")\n",
    "        \n",
    "        # Outlier visualization\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        plt.plot(self.df['date'], self.df[self.target], alpha=0.6, label='Consumption')\n",
    "        plt.axhline(y=lower_bound, color='red', linestyle='--', \n",
    "                   label=f'Lower bound ({lower_bound:.0f} GWh)')\n",
    "        plt.axhline(y=upper_bound, color='red', linestyle='--', \n",
    "                   label=f'Upper bound ({upper_bound:.0f} GWh)')\n",
    "        plt.scatter(outliers_low['date'], outliers_low[self.target], \n",
    "                   color='red', s=50, label='Low outliers', zorder=5)\n",
    "        plt.scatter(outliers_high['date'], outliers_high[self.target], \n",
    "                   color='orange', s=50, label='High outliers', zorder=5)\n",
    "        \n",
    "        plt.title('Consumption with Outliers Highlighted', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Date', fontsize=12)\n",
    "        plt.ylabel('Consumption (GWh)', fontsize=12)\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        self.save_plot('outliers_analysis.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def correlation_analysis(self):\n",
    "        \"\"\"Analyze correlations between features.\"\"\"\n",
    "        self.section_header(\"6. CORRELATION ANALYSIS\")\n",
    "        \n",
    "        # Select numeric columns\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        # Remove temporal features we created\n",
    "        exclude_cols = ['year', 'month', 'day_of_week', 'is_weekend']\n",
    "        numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "        \n",
    "        if self.target in numeric_cols:\n",
    "            # Calculate correlation with target\n",
    "            corr_with_target = self.df[numeric_cols].corr()[self.target].abs().sort_values(ascending=False)\n",
    "            \n",
    "            print(f\"TOP 20 FEATURES CORRELATED WITH TARGET:\")\n",
    "            for i, (feat, corr) in enumerate(corr_with_target.head(20).items(), 1):\n",
    "                print(f\"  {i:2d}. {feat:40s}: {corr:.4f}\")\n",
    "            \n",
    "            # Select top features for heatmap\n",
    "            top_features = corr_with_target.head(15).index.tolist()\n",
    "            \n",
    "            # Correlation matrix\n",
    "            corr_matrix = self.df[top_features].corr()\n",
    "            \n",
    "            # Heatmap\n",
    "            fig, ax = plt.subplots(figsize=(14, 12))\n",
    "            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "            sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "                       cmap='coolwarm', center=0, square=True, linewidths=1,\n",
    "                       cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
    "            ax.set_title('Correlation Heatmap - Top 15 Features', \n",
    "                        fontsize=14, fontweight='bold', pad=20)\n",
    "            plt.tight_layout()\n",
    "            self.save_plot('correlation_heatmap.png')\n",
    "            plt.close()\n",
    "    \n",
    "    def weather_impact_analysis(self):\n",
    "        \"\"\"Analyze weather impact on consumption.\"\"\"\n",
    "        self.section_header(\"7. WEATHER IMPACT ANALYSIS\")\n",
    "        \n",
    "        weather_cols = [col for col in self.df.columns if any(w in col.lower() \n",
    "                       for w in ['temp', 'precipitation', 'wind', 'radiation', 'humidity', 'rain', 'solar'])]\n",
    "        \n",
    "        if len(weather_cols) == 0:\n",
    "            print(\"No weather columns found in dataset\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(weather_cols)} weather related features\")\n",
    "        \n",
    "        # Create subplots for top weather variables\n",
    "        weather_cols_filtered = [c for c in weather_cols if c != self.target][:9]\n",
    "        \n",
    "        n_cols = 3\n",
    "        n_rows = (len(weather_cols_filtered) + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 5*n_rows))\n",
    "        axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "        \n",
    "        for idx, col in enumerate(weather_cols_filtered):\n",
    "            if idx < len(axes):\n",
    "                valid_data = self.df[[col, self.target]].dropna()\n",
    "                if len(valid_data) > 0:\n",
    "                    axes[idx].scatter(valid_data[col], valid_data[self.target], \n",
    "                                    alpha=0.3, s=10, color='steelblue')\n",
    "                    \n",
    "                    # Add trend line\n",
    "                    z = np.polyfit(valid_data[col], valid_data[self.target], 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    x_trend = np.linspace(valid_data[col].min(), valid_data[col].max(), 100)\n",
    "                    axes[idx].plot(x_trend, p(x_trend), \"r--\", linewidth=2, alpha=0.8)\n",
    "                    \n",
    "                    # Calculate correlation\n",
    "                    corr = valid_data[col].corr(valid_data[self.target])\n",
    "                    \n",
    "                    axes[idx].set_xlabel(col.replace('_', ' ').title(), fontsize=10)\n",
    "                    axes[idx].set_ylabel('Consumption (GWh)', fontsize=10)\n",
    "                    axes[idx].set_title(f'{col[:30]}\\nCorr: {corr:.3f}', fontsize=10, fontweight='bold')\n",
    "                    axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(len(weather_cols_filtered), len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self.save_plot('weather_impact.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Print weather correlations\n",
    "        print(\"\\nWEATHER CORRELATIONS WITH CONSUMPTION:\")\n",
    "        for col in weather_cols_filtered:\n",
    "            if col in self.df.columns and self.target in self.df.columns:\n",
    "                corr = self.df[col].corr(self.df[self.target])\n",
    "                print(f\"  {col:40s}: {corr:+.4f}\")\n",
    "\n",
    "    def run_full_analysis(self):\n",
    "        \"\"\"Run complete EDA pipeline.\"\"\"\n",
    "        try:\n",
    "            self.basic_statistics()    \n",
    "            self.temporal_analysis()\n",
    "            self.distribution_analysis()\n",
    "            self.outlier_analysis()\n",
    "            self.correlation_analysis()\n",
    "            self.weather_impact_analysis()\n",
    "            \n",
    "            print(\"\\n\" + \"*\" * 80)\n",
    "            print(\"EXPLORATORY DATA ANALYSIS COMPLETED SUCCESSFULLY\")\n",
    "            print(\"*\" * 80)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n Error during EDA: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    # Analyze original dataset (base)\n",
    "    DATA_PATH = \"data/processed/dataset_base.csv\"  \n",
    "    OUTPUT_DIR = \"code/Exploratory_result\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"EXPLORATORY DATA ANALYSIS - Energy Consumption Forecasting for Portugal Using Ensemble Machine Learning\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Dataset: {DATA_PATH}\")\n",
    "    print(f\"Output:  {OUTPUT_DIR}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        eda = ExploratoryAnalysis(DATA_PATH, OUTPUT_DIR)\n",
    "        eda.run_full_analysis()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nERROR: Dataset not found at {DATA_PATH}\")\n",
    "        print(\"\\n Make sure the dataset exists:\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.899942,
   "end_time": "2026-01-07T09:52:30.107052",
   "environment_variables": {},
   "exception": null,
   "input_path": "/Users/graciano/UA/Fundamentos de Aprendizagem Automática/2025/P2_ML2026_109557/code/eda.ipynb",
   "output_path": "/Users/graciano/UA/Fundamentos de Aprendizagem Automática/2025/P2_ML2026_109557/code/Exploratory_result/eda_20260107.ipynb",
   "parameters": {},
   "start_time": "2026-01-07T09:52:26.207110",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}